{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRC8U-0hRq9S",
        "outputId": "71f23ca1-03dd-4d02-b78f-071eb7130240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers fugashi ipadic\n",
        "!pip install demoji\n",
        "!pip install neologdn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEeC3MpHwG2J",
        "outputId": "41af0060-3251-4a0c-af97-f864148669a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting fugashi\n",
            "  Downloading fugashi-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (600 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.9/600.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Building wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=3d304ab0d812de48cf4904395c840daaee8be30e7d14ce0cbc3cdc0b2561cead\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic, fugashi\n",
            "Successfully installed fugashi-1.3.0 ipadic-1.0.0\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n",
            "Collecting neologdn\n",
            "  Downloading neologdn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: neologdn\n",
            "  Building wheel for neologdn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neologdn: filename=neologdn-0.5.2-cp310-cp310-linux_x86_64.whl size=219146 sha256=125c19832f51f3af8ae0cbac34ed052024f5261b509ac7a1491a878895edd323\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/87/af/2a36d74f666a8428943b70d71c5e9dd740435bf671f210672c\n",
            "Successfully built neologdn\n",
            "Installing collected packages: neologdn\n",
            "Successfully installed neologdn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "歌詞データ　スクレイピング"
      ],
      "metadata": {
        "id": "PLI4TkSoR3dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ライブラリインポート\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "#取得したデータを格納するデータフレームを作成\n",
        "df_list = []\n",
        "df = pd.DataFrame(columns=['artist','title','text'])\n",
        "\n",
        "#Uta-Net先頭URL\n",
        "base_url = 'https://www.uta-net.com'\n",
        "#King Gnuの歌詞一覧ページURL\n",
        "artist_list = [{\"name\":\"King Gnu\", \"url\":'https://www.uta-net.com/artist/23343/'},\n",
        "        {\"name\":\"Official髭男dism\", \"url\":'https://www.uta-net.com/artist/18093/'},\n",
        "        {\"name\":\"米津玄師\", \"url\":'https://www.uta-net.com/artist/12795/'},\n",
        "        {\"name\":\"back number\", \"url\":'https://www.uta-net.com/artist/8613/'},\n",
        "        {\"name\":\"Mrs. GREEN APPLE\", \"url\":\"https://www.uta-net.com/artist/18526/\"}\n",
        "        ]\n",
        "\n",
        "for i in range(0, len(artist_list)):\n",
        "  #歌詞一覧ページのHTML取得\n",
        "  response = requests.get(artist_list[i][\"url\"])\n",
        "  soup = BeautifulSoup(response.text, 'lxml')\n",
        "  links = soup.find_all('td', class_='sp-w-100 pt-0 pt-lg-2')\n",
        "  #歌詞ページより、情報を取得\n",
        "  for link in links:\n",
        "    a = base_url + (link.a.get('href'))\n",
        "\n",
        "    #歌詞ページよりHTMLを取得\n",
        "    response_a = requests.get(a)\n",
        "    soup_a = BeautifulSoup(response_a.text, 'lxml')\n",
        "    #アーティスト名取得\n",
        "    # artist = soup_a.find('h3').text.replace('\\n','')\n",
        "    #アーティストラベル\n",
        "    artist = i\n",
        "    #title取得\n",
        "    title = soup_a.find('h2').text\n",
        "    #歌詞取得\n",
        "    text = soup_a.find('div', itemprop='lyrics').text.replace('\\n','')\n",
        "    text = text.replace('この歌詞をマイ歌ネットに登録 >このアーティストをマイ歌ネットに登録 >','')\n",
        "\n",
        "    #取得したデータフレームに追加\n",
        "    temp_df = pd.DataFrame([[artist],[title],[text]], index=df.columns).T\n",
        "    df = df.append(temp_df, ignore_index=True)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fTz-O1OR5Ky",
        "outputId": "ab8b9ffc-c4a5-453e-8a8a-e1410bf22876"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d382732595fb>:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(temp_df, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    artist            title                                               text\n",
            "0        0          ):阿修羅:(  この人生たった一度切り発火する脳味噌が相方遊ぶ事も仕事の内君だけの素晴らしき日々修羅修羅修羅...\n",
            "1        0          あなたは蜃気楼  ひらりと翻したの風の吹くまま思惑、するり逃れてニヒルに駆けるキラリとこぼれ落ちた涙と思い出を...\n",
            "2        0                泡  消えたの泡となり消えたのいつの間にかわかってりゃもっとずっと一層清らかに溶け合ったのにねこの...\n",
            "3        0              雨燦々  選べよ　変わりゆく時代を　割り切れなくともこの瞬間この舞台を　生き抜くから手答えの無い　今日...\n",
            "4        0           IKAROS  笑ってよ、どうでも良くなる程燃やし切ってよ、諦めがつく程に決して微塵の虚しささえ残らぬほどに...\n",
            "..     ...              ...                                                ...\n",
            "438      4  Log (feat.坂口有望)  「誰にも言えないこと」僕は何個だろう？・嘘をついたキリがないな今日も手元にあるのはただ隠した...\n",
            "439      4          ロマンチシズム  「あなたって人はどんな人？」そんな風に聞けたらな背中押される夏の日には鮮やかに揺れる花になろ...\n",
            "440      4       Loneliness  死にたい今日も仕方がないでしょう？誰かにバレてしまう前にドス黒い夜に呑まれてしまう前に縛り付...\n",
            "441      4                私  空は深く澄んでて　息は白くて私は確かに此処で生きている私は昔から涙脆くて貴方はその度に側で笑...\n",
            "442      4             私は最強  さぁ、怖くはない不安はない私の夢は　みんなの願い歌唄えば　ココロ晴れる大丈夫よ　私は最強私の...\n",
            "\n",
            "[443 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "データセット作成"
      ],
      "metadata": {
        "id": "lUllCDKLvsbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertJapaneseTokenizer\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_val_df = df\n",
        "\n",
        "class CreateDataset(Dataset):\n",
        "  def __init__(self, tokenizer, df):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    text = self.df.at[index, \"text\"]\n",
        "    encoding = self.tokenizer(text, return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
        "    encoding = {key: torch.squeeze(value) for key, value in encoding.items()}\n",
        "    encoding[\"labels\"] = self.df.at[index, \"artist\"]\n",
        "    return encoding\n",
        "\n",
        "dataset = CreateDataset(tokenizer, train_val_df)\n",
        "print(len(dataset))\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [310, len(dataset)-310])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
        "print(len(train_dataset), len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gvcZKOkvvfG",
        "outputId": "6d27223d-af28-4840-f0ef-a59e037a9a92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "443\n",
            "310 133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習"
      ],
      "metadata": {
        "id": "TpYjPehov0oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "device = \"cuda\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=5).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "import numpy as np\n",
        "EPOCH = 12\n",
        "for epoch in range(EPOCH):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  for i, batch in enumerate(train_dataloader):\n",
        "    batch = {key: value.to(device) for key, value in batch.items()}\n",
        "    output = model(**batch)\n",
        "    loss = output.loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss\n",
        "\n",
        "  model.eval()\n",
        "  labels_list, outputs_list = [], []\n",
        "  for i, batch in enumerate(val_dataloader):\n",
        "    batch = {key: value.to(device) for key, value in batch.items()}\n",
        "    labels_list = np.concatenate([labels_list, batch[\"labels\"].cpu().detach().numpy()])\n",
        "    output = model(**batch)\n",
        "    output = output.logits.argmax(axis=1).cpu().detach().numpy()\n",
        "    outputs_list = np.concatenate([outputs_list, output])\n",
        "\n",
        "  accuracy = sum(outputs_list == labels_list) / len(outputs_list) * 100\n",
        "  print(f\"epoch: {epoch + 1}, train_loss: {round(train_loss.item(), 1)}, accuracy: {round(accuracy, 1)}% {sum(outputs_list == labels_list)}/{len(outputs_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wkgWbmVv2d7",
        "outputId": "3a04bc00-5c58-4bd0-8475-72ad1d0a549b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, train_loss: 59.3, accuracy: 43.6% 58/133\n",
            "epoch: 2, train_loss: 50.9, accuracy: 54.9% 73/133\n",
            "epoch: 3, train_loss: 38.3, accuracy: 60.2% 80/133\n",
            "epoch: 4, train_loss: 25.3, accuracy: 60.9% 81/133\n",
            "epoch: 5, train_loss: 15.4, accuracy: 63.2% 84/133\n",
            "epoch: 6, train_loss: 12.5, accuracy: 56.4% 75/133\n",
            "epoch: 7, train_loss: 11.8, accuracy: 69.9% 93/133\n",
            "epoch: 8, train_loss: 5.8, accuracy: 72.9% 97/133\n",
            "epoch: 9, train_loss: 3.2, accuracy: 77.4% 103/133\n",
            "epoch: 10, train_loss: 2.2, accuracy: 77.4% 103/133\n",
            "epoch: 11, train_loss: 1.5, accuracy: 77.4% 103/133\n",
            "epoch: 12, train_loss: 1.1, accuracy: 78.2% 104/133\n"
          ]
        }
      ]
    }
  ]
}